# Weather Data Preparation and Wrangling

This project uses weather and pollution data from remotely sensed satellite imagery, but also ground based sensors maintained by the EPA and FAA to model air quality in the Midwest. Using the ground sensors, the team can determine how accurate the models are at predicting air quality. This chapter focuses on how weather and pollution data from ground sensors was downloaded and prepared for use in refining air quality models.

## Querying EPA Data in R

EPA data was seamlessly imported into R using a custom package, named **epadata**. The package takes advanatge of the [EPA AQS DataMart API](https://aqs.epa.gov/aqsweb/documents/data_mart_welcome.html) to load data in R as data.frame objects with only a couple lines of code. It allows users to query for sensor data across multiple years, states, and air quality variables. Let's get started by downloading the package, then we'll do a walkthrough of the **epadata** package.

```{r download.epadata, message = FALSE, warning = FALSE}
devtools::install_url('https://uchicago.box.com/shared/static/jwpi7l89t58lm1sukit2tvcdavs5sz5t.zip', quiet = TRUE)
library(epadata)
```

### Getting Started

This section describes the process for querying EPA sensor data using the **epadata** package. For more information on how each function works, please reference the package documentation. 

#### Registering for an API Key {-}
For first time users of the AQS DataMart API, you must first register your email to recieve an API key. (Users who already have a DataMart API key, please skep to the next step). The API key is a required input for all querying functions in the **epadata** package. Obtaining a key is made simple by calling the *API.signup()* function and inputting your own email address.

```{r API.signup, eval = FALSE}
API.signup('YourEmailHere@uchicago.edu')
```

Save your API key from the email confirmation for future reference. In case you don't recieve an email, verify that your email address was typed correctly, and check your spam folder.

```{r API.details, echo = FALSE}
email = 'lmenendez@uchicago.edu'
key = 'tealmouse67'
```

#### Your First Data Query {-}
Below, we'll query for PM2.5 data across Indiana and Wisconsin for the years 2017 and 2018 using the *EPA.daily.summary()* function. This function will return a data.table containing a record of the daily average of the selected variable for all sensors with **available** data during the selected year and selected state. 

```{r first.query}
PM25.data = EPA.daily.summary(email = email,
                                  key = key,
                                  year.range = 2017:2018,
                                  state.FIPS.list = list(18, 55), 
                                  pollutant.code = 88502)

head(PM25.data)
```
Notice how for each each data request, you must specify which **timeframe** (year.range = 2017:2018), **states** (state.FIPS.list = list(18, 55)), and which **pollutant/weather variable** (pollutant.code = 88502) to query for. Inputting this data might not be strightforward at first, but the package has some other built-in functions to help you. 

### Selecting timeframes, states, and pollutant/weather variables
- **Timeframes**: This is the simplest variable to select, as it is simply a number range representing the year. More granular timescales can be extracted from queried data if needed. 

``` {r years, eval = FALSE}
year.range = 2014:2018 # Query for data in years 2014, 2015, 2016, 2017, and 2018
year.range = 2016:2016 # Query for data in 2016 only
```

- **States**: You must input the state(s) [FIPS](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code) code because the DataMart API uses this as a parimary key for all U.S. states.

```{r states, eval = FALSE}
state.FIPS.list = list(18, 55) # Query for data in Indiana (18) and Wisconsin (55)
state.FIPS.list = list(17) # Query for data in Illinois (17) only.
```

If you don't know the FIPS code associated with the State you're querying for, use the following function to find it. This function will return a data table containing the FIPS code for each State. 

```{r FIPS}
FIPS.codes = state.list(email, key)

head(FIPS.codes)
```

- **Pollutant/Weather Variable**: The EPA has a large bank of variables to choose from. Variables are first categorized as classes. A listing of available classes can be quried from the API as shown below. Each class has a unique code assigned to it.

```{r pollutant.class}
class.codes = class.list(email, key)

head(class.codes)
```

Using your chosen class code, you can find the individual variables within each class. Alternatively, you can also query for all variables, but this is not recommended as the list is very large. 

```{r pollutant.variables}
variable.codes.1 = variable.list(email, key, class = 'CORE_HAPS') # Search for variables in the Urban Air Toxic Pollutants class.

head(variable.codes.1)
```
```{r pollutant.varialbes.all, eval = FALSE}
variable.codes.2 = variable.list(email, key, class = 'ALL') # Returns all available EPA vairables. NB: This will create a very large data table and could take some time to load.
```

#### Advanced Querying {-}
- **Fetching Daily Summaries for Multiple Variables** in a single query is not recommended. This is due to long processing times regardless of internet bandwidth/speed and large output data tables. However, you may choose to do this usign the **apply** family of functions, as shown below.

```{r advanced.multi, eval = FALSE}
pollutant.codes = list(88502, 81102) # Two pollutant codes, one for each pollutant variable

pollutant.data = lapply(pollutant.codes, FUN = function(x){
    EPA.daily.summary(email = email,
                      key = key,
                      year.range = 2017:2018,
                      state.FIPS.list = list(18, 55), 
                      pollutant.code = x) 
    
}) %>%  do.call("rbind", .)

# Verifying that multiple pollutant variables were chosen
unique(pollutant.data$parameter)
```

### Sensor Locations and Metadata
The **epadata** package also allows you to query for station location and other sensor-related metadata. The function below does this for you, and has a similar usage to the daily summary function.

```{r locations}
sensors = EPA.monitor.locations(email = email,
                                    key = key,
                                    year.range = 2017:2018,
                                    state.FIPS.list = list(18, 55), 
                                    pollutant.code = list(88502, 81102))

plot(sensors)
```

By default, the function will return a spatial object. However, you can forego the spatial data component and keep only the attribute table by changing spatial to FALSE as shown below.

```{r locations-csv}
EPA.sensors.df = EPA.monitor.locations(email = email,
                                    key = key,
                                    year.range = 2017:2018,
                                    state.FIPS.list = list(18, 55), 
                                    pollutant.code = list(88502, 81102),
                                    spatial = FALSE)

EPA.sensors.df[1:2,]
```

## Querying for FAA Data in R
FAA weather data gathered from the [Automated Surface Observing System (ASOS)](https://en.wikipedia.org/wiki/Automated_airport_weather_station) can be imported using the **riem** package. This package, created by [ROpenSci](https://ropensci.org/) queries weather data from the [Iowa Environmental Mesonet](https://mesonet.agron.iastate.edu/ASOS/), an online portal for international ASOS data maintained by Iowa State University. First, let's load the package.

```{r download.riem}
devtools::install_github('ropensci/riem')
library(riem,quietly = TRUE)
```

### Simple Data Query
Below is an R code snippet that performs the simplest weather data query possible in the **riem** package. It specifies a particular weather station using an airport code and a date range to query for. The output is a tibble table of raw ASOS weather data.

``` {r simple.riem.query}
SFO.weather = riem_measures(station = 'KSFO', date_start = "2014-01-01", date_end = '2014-01-02')
head(SFO.weather)
```
The outputted table shows weather data for a 24-hour period on January 1st, 2014 at the San Francisco International Airport. The *valid* column species when each weather report was generated, typically at 1-hour intervals. The *tmpf* and *dwpf* columns give the ambient air temperature and dew point in Fahrenheit (ÂºF). Other important variables in our project include air pressure (*alti*), measured in inches of mercury (in.Hg), and visibility (*vsby*) in miles. For more information on all available varibles, see Iowa State's [Documentation](https://mesonet.agron.iastate.edu/request/download.phtml).

Next, we will apply this function at a large scare across multiple sensors and timescales.


### Selecting Which Sensors to Query
The FAA collects weather data at hourly intervals for each meteorological station, with some stations  providing half-hour intervals. Even querying for short periods of time can yield large amounts of data. To optimise performance, we want to only query data from stations in our study area.

#### Finding Sensors by State {-}
In our project, we focus on certain counties in Illinois, Indiana, and Wisconsin, so we are interested in finding the sensors within that study area. The first step is to query the locations of all weather stations in Illinois, Indiana, and Wisconsin using the **reim** package. In the example below, we query for sensors in the Illinois ASOS sensor network. 

```{r IL.query}
riem_stations(network = 'IL_ASOS')
```

To query for data across multiple states, we are going the apply the *riem_stations* function to a list of weather station networks, as shown below.

```{r IL.IN.WI.query, message = FALSE}
networks = list('IL_ASOS', 'IN_ASOS', 'WI_ASOS')

library(dplyr, quietly = TRUE)
station.locs = lapply(networks, riem::riem_stations) %>% 
        do.call(rbind, .) # Creates a single data table as output

head(station.locs)

```

Note: You can find a list of state abbreviations by typing 'state.abb' in your R console. 

#### Converting Latitude and Longitude Coordinates to Spatial Data {-}
The data tables returned by the **riem** package must be converted to spatial data to determine which sensors are located in the study area. Since the lon/lat coordinates are already provided, the data table is easily converted to a spatial *sf* object.

```{r csv.to.spatial}
station.locs.sf = sf::st_as_sf(station.locs, coords = c("lon", "lat"), crs = 4326)

# Plot stations and study area boundaries to verify that the correct sensors were selected
plot(station.locs.sf$geometry)
plot(sf::st_read('https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson', quiet = TRUE)$geometry, border = 'red', add = TRUE)
```

We plot to results to verify that our query and data conversion process worked correctly. For reference, the boundaires of the study area is outlined in red. 


#### Selecting Sensors within a Study Area {-}
Next, we perform a spatial join to only keep the points located within the boundaries of our study area polygons. The spatial join is completed by the **sf** package, as shown below. For more information regarding spatial joins and spatial predicates, please see [this](https://gisgeography.com/spatial-join/) helpful blog post by GISgeography.com.

```{r sensor.join, message = FALSE}

# Loading study area boundaries
study.area = sf::st_read('https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson', quiet = TRUE)

study.sensors = sf::st_join(station.locs.sf, study.area, left = FALSE)

# Verify Spatial Join by Plotting
plot(study.area$geometry, border = 'red')
plot(study.sensors$geometry, add = TRUE)
title('Weather Stations Within the Study Area')

```

Now that we have a dataset of which weather stations we are interested in, we can query for the weather data associated with each station.

### Querying Weather Data for Multiple Stations
Again we use the *lapply* function in base R to execute the *riem_measures* function on a list of sensor IDs. This allows us to iterativelt query for weather data from each individual sensor in a list. In the code snippet below, we take the study sensors obtained previously and query for a single day's worth of weather data.

```{r multi.locs.query, message = FALSE}
library(dplyr, quietly = TRUE)

weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = "2014-01-01", date_end = "2014-01-02")}) %>% 
        do.call(rbind, .) # Creates a single data table as output

head(weather.data)
```

#### Missing Weather Stations {-}
Some weather stations are unable to provide data during the requested time period, so a warning message is returned to the R console. You can see which ones are not active by comparing the *study.sensors* IDs and the *weather.data* station IDs, as shown below.

``` {r find.missing.stations}
which(!(study.sensors$id %in% weather.data$station)) %>% study.sensors[.,]
```

#### Querying Large Weather Datasets {-}
Use caution when querying for a large amount of data. Data tables can easily become unwieldy after querying for a large number of weather stations across a wide time scale. The code snippet below downloads all ASOS weather data for sensors in our study area from January 1st 2014 to December 31st 2018. It has approximately 4.8 Million records and takes 6-10 minutes to download.

```{r large.query, eval = FALSE}
weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = "2014-01-01", date_end = "2018-12-31")}) %>% 
        do.call(rbind, .) # Creates a single data table as output

which(!(study.sensors$id %in% weather.data$station)) %>% study.sensors[.,]
```

## Using Weather Data to Interpolate a Raster Surface
To refine our models, we need to know or estimate meteorological variables across our entire study area. However, the downloaded weather data is only accurate at the exact sensor location where the measurement was taken. Therefore, we propose to linearly interpolate meteorological variables. The method used is an Inverse Distance Weighting for a raster resolution of 1 km, implemented in R as described below. 

### Your First Interpolation
This section describes how to create interpolated raster surfaces using the purpose-built **sensor2raster** package. First, let's download and install the package.

``` {r load.sensor.package, message = FALSE, warning = FALSE}
devtools::install_url('https://uchicago.box.com/shared/static/u57vxg90ytieu86ow3m8xats3ebmfek4.tar.gz', quiet = TRUE)
library(sensor2raster)

```

#### Workflow Execution {-}
Next, we run the *Sensor.to.Raster* function on a sample EPA dataset of PM2.5 readings across Indiana Illinois, and Wisconsin in 2017. The function takes the raw output from an EPA or FAA data query, a data type ('EPA' or 'FAA' only) to perform the IDW analysis. An input of county boundaries defines the extent of the raster output. Meanwhile, the reference.grid input allows the user to input a vector-format grid to improve the readability of the output.

```{r sensor.to.raster.1, warning = FALSE}
raster = Sensor.to.Raster(Sensor.data = sample.data, 
                          data.type = 'EPA', 
                          reference.grid = km.grid, 
                          counties = sample.counties)
```

#### Output Interpretation {-}
The *Sensor.to.Raster* function outputs four objects in a list. The first two elements contain Vector data products at Monthly and Quarterly time scales. The last two elements contain the equivalent product in Raster format.

##### Raster Product {-}
Below is the sample Raster output for the 1st Quarter and 1st Month of 2017. Notice how multiple list objects are subsetted in the ouput of *Sensor.to.Raster*

```{r sensor.to.raster.results1}
raster::plot(raster$Quarterly_Raster_Data$Q1.2017, main = 'Average Observed PM2.5 Levels in Q1 2017')
sp::plot(sample.counties, add = TRUE) # Adding county boundaries for reference
```
```{r sensor.to.raster.results2}
raster::plot(raster$Monthly_Raster_Data$M1.2017, main = 'Average Observed PM2.5 Levels in Jan 2017')
sp::plot(sample.counties, add = TRUE) # Adding county boundaries for reference
```

##### Vector Product {-}
The same data is available is a vecorized grid format using the same grid specified by the user as an input. In this format, each month or quarter is represented by as a data field in the attribute table. This not only makes the data more compact, but also allows for improved data analysis. Below, we show the attribute table for the first five grid cells.

```{r sensor.to.raster.results3}
as.data.frame(raster$Monthly_Vector_Data[1:5,])
```

### The IDW Workflow
The workflow is designed to take time-series data input for a **single** variable from a multitude of either EPA or FAA ASOS sensors, and create IDW surfaces at monthly and quarterly timescales. The final data product models the intensity of the chosen variable across a wide area.

For example, this workflow can take ASOS Temperature data gathered from multiple sensors across a wide-area and predict the temperature in each location across the area, even if a sensor is not located there.

The entire workflow is divided into five helper functions that are each called in sucession by the larger *Sensor.to.Raster* function. Three functions act upon the EPA or FAA data to wrangle it into an approprite format for the IDW interpolation. The interpolation is handeled by another helper function based off the *idw* function from the **gstat** package. Finally, the data is post-processed into manageble vector and raster products.






